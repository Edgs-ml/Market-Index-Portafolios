---
title: "Market Index Portafolio Code Copilation"
author: "Edgar Sigfrido Soto Aparicio y Juan Luis Gonzáles Jiménez"
date: "11/18/2021"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: 3
    doc: yes
    doc_depth: 3
---
# PARTE 1. INTRODUCCIÓN
## 1.1 MOTIVACIÓN

Con esta investigación queremos conocer si el metodo de reducción de dimensiones (PCA) de

queremos basarnos en estos indicadores para encontrar grupos de paises competitivos y estables en los cuales invertir. Con esta investigación buscamos descubrir si la aplicación de tecnicas de *Machine Learning*, como la reducción de dimensiones del *Principal Component Analysis* es efectivo para encontrar dichos grupos.

Verificar si el proceso de PCA 

## 1.2 PLANTEAMIENTO DEL PROBLEMA Y CONTEXTO
Nuestra investigación se basa en la modelación y la prueba de
## 1.3 PREGUNTA DE INVESTIGACIÓN
## 1.4 *SOLUTION OVERVIEW*

# PARTE 2. DEFINICIÓN DE PORTAFOLIOS A TRAVÉS DE AGRUPACIÓN DE PAISES CON PCA Y K-MEANS
Cargar las librerias
```{r include=FALSE message=FALSE}
library(tidyverse)
library(readxl)
library(textshape) #"Tools for Reshaping Text". Usado en columns_to_rownames
library(broom)
library(plotly)
library(scales)
library(caTools)
library(caret)
library(cluster)
library(factoextra) #Para graficar K-Means y PCA
library(psych) #Usado por su funsión de crar matices de correlaciones de colores
library(stats) #Para hacer el PCA
library(naniar) #Para limpiar las bases de datos
library(fBasics) #Analisis estadistico
library(PerformanceAnalytics)
library(statmod)
library(knitr)
library(stargazer)
library(kableExtra)
library(ggpubr)
library(ggdist)
library(ggExtra)
library(ggbeeswarm)
```
## 2.1. Descripción de los índices seleccionados
```{r include=FALSE eval=FALSE warning=FALSE message=FALSE}
df <- read_excel("APLHA/ALPHA_1/ALPHA_1.1/1.1.1PCA_Codes/Criterios-Unificado (Datos para PCA).xlsx")
df_plot <- read_excel("APLHA/ALPHA_1/ALPHA_1.1/1.1.1PCA_Codes/Criterios-Unificado (Datos para PCA).xlsx")
#df2: eliminar a China y Estados Unidos
df2 <- df %>%
  subset(Country!="China" & Country!="United States")
#df: Data Frame with all variables and observations
df <- column_to_rownames(df, loc = 1)
#df1: Data frame without GDP and with all the countries
df1 <- df[,-4]
#df2: Data Frame without China and USA and with GDP
df2 <- column_to_rownames(df2, loc = 1)

kable(head(df)) #Tabla de los primero paises en Latex
```
### GDP dgraphs
```{r}
bxp_gdp <- df_plot %>%
  ggplot(aes(x=GDP, y=0))+
  geom_boxplot()+
  geom_quasirandom()+
  geom_label(data=df_plot %>% 
               filter(GDP>1.0e+13), aes(label=Country))

#vio_gdp <- df_plot %>%
#  ggplot(aes(x=GDP, y=GCI))+
#  geom_violin()+
#  geom_label(data=df_plot %>% 
#               filter(GDP>1.0e+13), aes(label=Country))

hist_gdp <- df_plot %>%
  ggplot(aes(x=GDP))+
  geom_histogram(bins=100)+
  geom_label(data = df_plot %>%
               filter(GDP>1.0e+13), aes(x=GDP,y=Country, 
                                        label=Country))

#stat_desc_gdp <- kable(describe(df_plot$GDP), )

ggarrange(bxp_gdp, hist_gdp,
          labels = c("Boxplot", "Histogram"),
          ncol = 2,
          nrow = 1)
```
### ECI graphs
```{r}
bxp_eci <- df_plot %>%
  ggplot(aes(x=ECI, y=0))+
  geom_boxplot()+
  geom_quasirandom()

vio_eci <- df_plot %>%
  ggplot(aes(x=ECI, y=0))+
  geom_violin()

ggarrange(bxp_eci, vio_eci, 
          labels = c("A", "B"),
          ncol = 2,
          nrow = 1)
```
### EDBI graphs
```{r}
bxp_edbi <- df_plot %>%
  ggplot(aes(x=EDBI, y=0))+
  geom_boxplot()+
  geom_quasirandom()
vio_edbi <- df_plot %>%
  ggplot(aes(x=EDBI, y=0))+
  geom_violin()

ggarrange(bxp_edbi, vio_edbi, 
          labels = c("A", "B"),
          ncol = 2,
          nrow = 1)
```
### ECI graphs
```{r echo=FALSE}
bxp_gci <- df_plot %>%
  ggplot(aes(x=GCI, y=0))+
  geom_boxplot()+
  geom_quasirandom()
vio_gci <- df_plot %>%
  ggplot(aes(x=GCI, y=0))+
  geom_violin()

ggarrange(bxp_gci, vio_eci, 
          labels = c("A", "B"),
          ncol = 2,
          nrow = 1)
```
```{r echo=FALSE}
ggarrange(bxp_eci, bxp_gci, bxp_edbi, bxp_gdp,
          labels = c("ECI Boxplot", "GCI Boxplot", "EDBI Boxplot", "GDP Boxplot"),
          ncol = 2,
          nrow = 2)
```

```{r echo=FALSE}
tabledf <- describe(df)

kable(tabledf, format = "latex") %>%
  kable_styling(latex_options = "scale_down", font_size = 5)
```
```{r echo=FALSE}
kable(tabledf,
      caption = "Descripción Estadistica de los índicadores")
```
```{r, echo=FALSE}
describe(df1)
```

## 2.2. PCA
### 2.2.1. Matriz de correlaciones
```{r, echo=FALSE}
cor.plot(df, main = "Correlación de GCI, ECI, EDBI, y GDP")
cor.plot(df1, main = "Correlación de GCI, ECI, y EDBI")
```
Todas las variables tienen una significante correlación entre si, excepto por el GDP. La correlación entre GDP y las otras variables aumenta cuando eliminamos a los outliers

### 2.2.2. Creación de los vectores de PCA de cada Data Frame
```{r, echo=FALSE}
pca_df <- prcomp(df)
pca_df1 <- prcomp(df1)
summary(pca_df)
summary(pca_df1)
```

Importancia del componente principal
porcentaje de la varianza que se explica con el PCA1

### 2.3. Crear una nueva data frame con los 3 Componentes principales como variables y ordenarlos de forma descendiente por el PC1
```{r}
df_PC1234 <- cbind(df, pca_df$x)
df_PC1234_Descent <- df_PC1234 %>%
  arrange(desc(PC1))

df1_PC123 <- cbind(df1, pca_df1$x)
df1_PC123_Descent <- df1_PC123 %>%
  arrange(desc(PC1))

kable(head(df_PC1234_Descent, 5), 
      format = "latex")
kable(head(df1_PC123_Descent, 5),
      format = "latex")
```

## 3. Graficas de PCA
#### 3.1.1. Mapa Cartesiano con PC1 y PC2 como ejes para visualizar la posición de los paises.
```{r, eval=FALSE, warning=FALSE}
fviz_pca_ind(pca_df,
             repel = FALSE,
             title = "Lugar de Países dentro de los Componentes Principales 1 y 2 [con GDP]")

fviz_pca_ind(pca_df1,
             repel = TRUE,
             title = "Place of each country in a PC1 and PC2 Map [Without GDP]")
```
### 3.2. Gráfica de individuos y variables.
```{r}
Biplot_fvis_df <- fviz_pca_biplot(pca_df)

Biplot_fvis_df1 <- fviz_pca_biplot(pca_df1)
```
### 3.3. Contribución de varianza de variables y de Componentes Principales
```{r}
Contrib_var_fvis_df <- fviz_contrib(pca_df, choice = "var",
             title = "Porcentaje de Varianza Contribuida con GDP y todas las Observaciones",
             addlabels = TRUE)

Contrib_var_fvis_df1 <- fviz_contrib(pca_df1, choice = "var",
             title = "Percentage of Variance contribution Without Variable GDP",
             addlabels = TRUE)
```
### 3.4. Porcentaje de varianza explicada por cada Componente Principal
```{r}
Porcen_var_fvis_df <- fviz_screeplot(pca_df,
               title = "4 Principal Components With USA & China",
               addlabels = TRUE)

Porcen_var_fvis_df1 <- fviz_screeplot(pca_df1,
               title = "3 Principal Components Without GDP",
               addlabels = TRUE) # Porcentaje de la varianza explicada con el PCA1
```
## 3.5.1 
```{r}
#PCA DF
ggarrange(Contrib_var_fvis_df, Porcen_var_fvis_df, Biplot_fvis_df,
          ncol = 2,
          nrow = 2)

#,labels = c("Porcentaje de Varianza Contribuida con GDP y todas las Observaciones",
#                     "4 Componentes Principales con Estados Unidos y China", 
#                     "Valor de Explicación del Componente Principal como Vector")

#PCA DF1
ggarrange(Contrib_var_fvis_df1, Porcen_var_fvis_df1, Biplot_fvis_df1,
          ncol = 2,
          nrow = 2)
```

### 3.6. Matriz de correlaciones con los Componentes Principales
```{r}
cor.plot(df_PC1234)

cor.plot(df1_PC123)
```

A partir de los resultados del PCA concluimos que debemos de excluir la variable GDP y mantener a Estados Unido y China. Ahora se hará un agrupamiento de la Data Frame 1 para encontrar los grupos de paises.

## 4. K-MEANS
Crear grupos de 5 paises con base en los 3 Componentes Principales de la Data Frame 1

```{r echo=FALSE}
# A partir del mapa hecho del PC1 y PC2 creamos un objeto para facilitar la extracción de
# sus valores
map <- fviz_pca_ind(pca_df1, 
                    repel = TRUE,
                    title=NULL)
#map Es opcional ver el resultado

# Creamos un Data Frame para luego poder usar sus valores para el Kmeans
names_map <- as.data.frame(map[["data"]][["name"]])
y_map <- as.data.frame(map[["data"]][["y"]])
x_map <- as.data.frame(map[["data"]][["x"]])
coordinates_map <- cbind(names_map, x_map, y_map)
colnames(coordinates_map) <- c("Country", "x","y")
row.names(coordinates_map) <- coordinates_map$Country
coordinates_map <- coordinates_map[,-1]

coordinates_map %>%
  ggplot(aes(x=x, y=y))+
  geom_point()+
  geom_text(aes(label=names_map$`map[["data"]][["name"]]`))

# Análisis previo del Kmeans

fviz_nbclust(coordinates_map,
             kmeans,
             method = "wss",
             k.max = 24)

fviz_nbclust(df1_scaled,
             kmeans,
             method = "gap_stat",
             k.max = 30)

# CORREGIR !!! PONER ARGUMENTO DE SIZE=5 PARA TENER LOS PORTAFOLIOS DE 5 PAISES

k_map <- kmeans(coordinates_map, 7)
fviz_cluster(k_map, 
             data = coordinates_map)

#List of countries by cluster

k_map_df <- as.data.frame(k_map$cluster)

k_map1 <- k_map_df %>%
  arrange(`k_map$cluster`)

k_map1 <- k_map_df %>%
  subset(`k_map$cluster`==7)

head(k_map1)
```


-----
Crear una nueva Data Frame con solo los valores de PC del df1
```{r}
# Opcional
# Este paso lo estamos haciendo ya al subset los grupos del Kmeans
Kdf1_Descent = subset(df1_PC123_Descent, select = c("PC1","PC2", "PC3"))
head(Kdf1_Descent, 5)
```
-------



# PARTE 3. ÍNDICES BRUSATILES DE LOS PAISES SELLECCIONADOS Y CREACIÓN DE PORTAFOLIOS

```{r, include=FALSE}
library(cluster)
library(factoextra) #Para graficar K-Means y PCA
library(psych) 
library(stats) #Para hacer el PCA
library(naniar) #Para limpiar las bases de datos
library(fBasics) #Analisis estadistico
library(aTSA) #Raiz Unitaria
library(tseries) #Raiz Unitaria
library(PerformanceAnalytics)
library(QuantPsyc) #Pruba multivariada
library(statmod)
library(ghyp) #Para hacer momentos estadisticos de la NIG
library(quantmod) #Para descargar datos
library(cramer) #Para la prueba de cramer 
```

## 3.1 Primer grupo
```{r include=FALSE}
Serie_de_datos_el_bueno_ <- read_excel("~/Documents/GitHub/Market-Index-Portafolios/iNDICES.xlsx", 
    col_types = c("date", "numeric", "numeric", 
        "numeric", "numeric", "numeric", "numeric", "numeric"))
g1 <- Serie_de_datos_el_bueno_
colnames(g1)<-c("Fecha", "SP", "OMX30","HSI","STI","DJI","KOSPI", "DAX")
g1<-textshape::column_to_rownames(g1,loc=1)
```

```{r}
g1 <- drop_na(g1)
retornos <- Return.calculate(g1,
                           method = "log")[-1,]
```
Estadisticas descriptivas
```{r}
Estg1 <- basicStats(retornos)
Estg1
```
## 3.2 Pruebas Estadisticas de los Retornos
### 3.2.1 Prueba de Augmented Dickey-Fuller
```{r}
adf_sti <- adf.test(retornos$SP)

adf_sti_table <- as.data.frame(c(adf_sti$p.value))
adf_sti_table <- adf_sti_table %>%
  mutate(method=adf_sti[["method"]])
adf_sti_table <- adf_sti_table %>%
  mutate(DF=adf_sti[["statistic"]][["Dickey-Fuller"]])



adf.test(retornos$SP)
adf.test(retornos$OMX30)
adf.test(retornos$HSI)
adf.test(retornos$STI)
adf.test(retornos$DJI)
adf.test(retornos$KOSPI)
adf.test(retornos$DAX)
```

### 3.2.2 Prubas de Kormogorov-Smirnov contra distribucion normal, creada con paramaetros de nuestra serie 
*Construccion de normal con parametros multivariados de la series*
```{r}
m <- mean(retornos$SP)
sd <- sd(retornos$SP)
len <- length(retornos$SP)
basenormal <- dnorm(len,m,sd)
```
normal con los parametros de nuestras series
```{r}
ks.test(retornos$SP, basenormal)
ks.test(retornos$OMX30, basenormal)
ks.test(retornos$HSI, basenormal)
ks.test(retornos$STI, basenormal)
ks.test(retornos$DJI, basenormal)
ks.test(retornos$KOSPI, basenormal)
ks.test(retornos$DAX, basenormal)
```
### 3.2.3 Pruba de normalidad multivariada
```{r}
mult.norm(retornos)$mult.test
```

```{r echo = FALSE}
plot(density(retornos$SP),
     col="red",
     ylim=c(0,60), 
     main="Distribuciones contra normal")+
  lines(density(retornos$OMX30),
     col="green",
     ylim=c(0,60))+
  lines(density(retornos$HSI),
     col="blue",
     ylim=c(0,60))+
  lines(density(retornos$STI),
     col="orange",
     ylim=c(0,60))+
  lines(density(retornos$DJI),
     col="pink",
     ylim=c(0,60)+  
  lines(density(retornos$KOSPI),
     col="purple",
     ylim=c(0,60))+
  lines(density(retornos$DAX),
     col="brown",
     ylim=c(0,60)))
```
# Construcción de la NIG
```{r}
#Parametros de la NIG
  NIG<-nigFit(retornos$SP)

#Agrupar parametros en un objeto
   a<-NIG@fit[["par"]]
   a<-data.frame(t(a))
   
#NIG aleatoria con parametors univariados de nuestra serie
   r = rnig(len,
            alpha = a$alpha, 
            beta = a$beta, 
            delta = a$delta,
            mu= a$mu)
   plot(density(r),
        col="black",
        main="NIG Univariada",
        sub="SP index")
   
#Pruba de Kormogorov univariada para NIG
ks.test(retornos$SP,r)
```

--------
```{r}
#Parametros para NIG Multivariada
multNIG <- fit.NIGmv(data=retornos,
                     silent=FALSE)

#Localizar parametros dentor de un obejto
Mom1NIGm <- multNIG@expected.value
Mom2NIGm <- multNIG@variance

#Construccion de la funcion NIG con nuestros  parametros de la funcion multivariada
Mnig <- rghyp(len, multNIG)
retornos1 <- as.matrix(retornos)

#Prueba cramer de comprobacion
  #Se buscan similitudes estadisticas
cramer.test(Mnig,
            retornos1,
            conf.level = .95)


```
### Graficas de NIG Multivariada
```{r echo = FALSE}
plot(density(Mnig),col="black",
     ylim=c(0,60),
     main="Distribuciones contra NIG multivariada")+
  lines(density(retornos$SP),col="red")+
  lines(density(retornos$OMX30),col="green")+
  lines(density(retornos$HSI),col="blue")+
  lines(density(retornos$STI),col="orange")+
  lines(density(retornos$DJI),col="pink")+
  lines(density(retornos$KOSPI),col="purple")+
  lines(density(retornos$DAX),col="brown")
```
----------

# PARTE 4. COMPARACIÓN DE PORTAFOLIOS NORMALES CONTRA NIG

## 4.1 Optimización de portafolio 1
*Optimizar portafolio 1 con NIG y con Nomral y comparar las curvas de Markowits para ver cual es mejor y comparar sus Sharps*

```{r include = FALSE}
library(quantmod)
library(PerformanceAnalytics)
library(PortfolioAnalytics)
library(DEoptim)
library(readxl)
library(fBasics)
library(ghyp)
library(PortfolioAnalytics)
```
```{r echo=FALSE include = FALSE}
Specs_Port_Norm <- portfolio.spec(retornos)

##### Add Constraints #####
Specs_Port_Norm <- add.constraint(Specs_Port_Norm,type = "full_investment")
Specs_Port_Norm <- add.constraint(Specs_Port_Norm,type="long_only")

##### Add Objective #####
Specs_Port_Norm <- add.objective(Specs_Port_Norm,type="risk",name="StdDev")
Specs_Port_Norm <- add.objective(Specs_Port_Norm,type='return',name='mean')
Specs_Port_Norm
```
#### Optimization
```{r}
Optimized_Port_Norm <- optimize.portfolio(retornosm,
                                      Specs_Port_Norm,
                                      trace = TRUE)
```
```{r}
chart.Weights(Optimized_Port_NORM,plot.type="barplot")
W_R_NORM <- extractWeights(Optimized_Port_NORM)
sum(W_R_NORM)
```
Porrtfolio analysis Normal Distribution
```{r}
Return_opt_NORM <- Return.portfolio(Portafolio_NORM,
                                    W_R_NORM)

table.AnnualizedReturns(Return_opt_NORM,
                        scale = 252,
                        geometric = FALSE)

Return.cumulative(Return_opt_NORM,
                  geometric = FALSE)

chart.RiskReward(Optimized_Port_NORM,
                 risk.col = 'StdDev',
                 return.col = 'mean',
                 chart.assets = T)
```

### Port NIG
##### Add Constraints #####
```{r}

Specs_Port <- add.constraint(Specs_Port,type="full_investment")
Specs_Port <- add.constraint(Specs_Port,type="long_only")



```
##### Add Objective #####
```{r}

Specs_Port <- add.objective(Specs_Port,type="risk",name="StdDev")
Specs_Port <- add.objective(Specs_Port,type='return',name='mean')
Specs_Port

covnig<-function(R,portfolio){
  a<-fit.NIGmv(R,silent=TRUE)
  COV<-a@variance
  mu<-a@expected.value
  mu<-matrix(mu,ncol = 1)
  resultado<-list(mu=mu,sigma=COV)
  return(resultado)
}

Optimized_Port_NIG <- optimize.portfolio(rendimientos,
                                         Specs_Port,
                                         momentFUN = covnig,
                                         optimize_method = "random",
                                         trace = TRUE)
```


```{r}
…
```





# Portafolio 2


# PARTE 3

### Portfolio Evaluation
```{r}
Return_opt3 <- Return.portfolio(Returns_3, W3)

table.AnnualizedReturns(Return_opt3,
                        scale = 252,
                        geometric = FALSE)
Return.cumulative(Return_opt3,
                  geometric = FALSE)

plot(cumsum(Return_opt3))

varRisk(Portfolio_3, W3)

DtaFrm_Portfolio2 <- as.data.frame(Portfolio_2)

chart.RiskReward(Optimized_Port3,
                 risk.col = 'StdDev',
                 return.col = 'mean',
                 chart.assets = TRUE)
combine.portfolios(Optimized_Port2)

chart.EfficientFrontierOverlay(Optimized_Port1,
                               risk.col = 'StdDev',
                               return.col = 'mean',
                               chart.assets = TRUE)
```
# Comparación
#--------------Graficas comparativas
```{r}
rand <- Optimized_Port1$random_portfolios
# 1794 portafolios de diferentes pesos al asar 

stdv <- Optimized_Port1$random_portfolio_objective_results[[1]]$objective_measures$StdDev

medias <- Optimized_Port1$random_portfolio_objective_results[[1]]$objective_measures$mean
```

```{r}
mediasNORM <- NULL
standDevNORM <- NULL
for (i in 1:1682) {
  mediasNORM[i] <- Optimized_Port_Normal$random_portfolio_objective_results[[i]]$objective_measures$mean
  standDevNORM[i] <- Optimized_Port_Normal$random_portfolio_objective_results[[i]]$objective_measures$StdDev
}

mediasNIG <- NULL
standDeviNIG <- NULL
for (i in 1:1768) {
  mediasNIG[i] <- Optimized_Port_NIG$random_portfolio_objective_results[[i]]$objective_measures$mean
  standDeviNIG[i] <- Optimized_Port_NIG$random_portfolio_objective_results[[i]]$objective_measures$StdDev
}
```
#### Fronteras
```{r}
fronteraNorm <- tibble(Volatility = standDevNORM, Expected_Return = mediasNORM)
fronteraNIG <- tibble(Volatility = standDeviNIG, Expected_Return = mediasNIG)

fronteraNorm %>%
  ggplot(aes(x=Volatility, y=Expected_Return))+
  geom_point(alpha=0.5, col="pink")+
  geom_point(data=tibble(Volatility = Optimized_Port_Normal$objective_measures$StdDev,
                         Expected_Return = Optimized_Port_Normal$objective_measures$mean),
             col="red", size=3)+
  geom_point(data=fronteraNIG, aes(x=Volatility, y=Expected_Return), alpha=0.2, col="blue")+
  geom_point(data=tibble(Volatility = Optimized_Port_NIG$objective_measures$StdDev, 
                         Expected_Return = Optimized_Port_NIG$objective_measures$mean), 
             alpha=1, col="darkgreen", size=3)
```



# PARTE 5. ¿SON LOS PORTAFOLIOS UN GRUPO DE INVERSIÓN?
A travez de formación de portafolios probar si un grupo de activos son un grupo de inversión o no.
# PARTE 6. MODELOS DE REGRESIÓN PARA EXPLICAR LAS RAZONES DE SHARP DE LOS GRUPOS



