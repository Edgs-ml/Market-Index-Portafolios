---
title: "Unsupervised Learning in R"
subtitle: "Course"
author: "Edgar Sigfrido Soto Aparicio"
date: "2023-05-15"
output: html_document
---

```{r, include=FALSE, echo=FALSE}
library(tidyverse)
library(datasets)
library(stats)
library(readr)
iriss <- iris[,1:2]
Pokemon <- read_csv("/Users/edgarsigfridosotoaparicio/Documents/GitHub/Notes-and-Courses/R/Machine Learning Fundamentals in R - Skill Track/Datasets/Pokemon.csv")
Names <- Pokemon$Name
Pokemon <- cbind(Pokemon, Names)
rownames(Pokemon) <- Pokemon$Names
Pokemon <- Pokemon[,c("Defense", "Speed")]
```

# CHAPTER 1. UNSUPERVISED LEARNING IN R
## Types of machine learning
There are three major types of machine learning. The first type is *unsupervised learning*. The goal of unsupervised learning is to find structure in unlabeled data. Unlabeled data is data without a target, without labeled responses. Contrast this with supervised learning. *Supervised learning* is used when you want to make predictions on labeled data, on data with a target. Types of predictions include regression, or predicting how much of something there is or could be, and classification which is predicting what type or class some thing is or could be. The final type is *reinforcement learning*, where a computer learns from feedback by operating in a real or synthetic environment.

## Unsupervised learning - clustering
Within unsupervised learning there are two major goals. The first goal is to find homogeneous subgroups within a population. As an example let us pretend we have a population of six people. Each member of this population might have some attributes, or features — some examples of features for a person might be annual income, educational attainment, and gender.

With those three features one might find there are two homogeneous subgroups, or groups where the members are similar by some measure of similarity.

Once the members of each group are found, we might label one group subgroup A and the other subgroup B. The process of finding homogeneous subgroups is referred to as clustering.

## Unsupervised learning - dimensionality reduction
Dimensionality reduction is often used to achieve two goals, in addition to finding patterns in the features of the data. Dimensionality reduction allows one to visually represent high dimensional data while maintaining much of the data variability. This is done because visually representing and understanding data with more than 3 or 4 features can be difficult for both the producer and consumer of the visualization. The third major reason for dimensionality reduction is as a preprocessing step for supervised learning. More on this usage will be covered later.

## k-means clustering algorithm
K-means is a clustering algorithm, an algorithm used to find homogeneous subgroups within a population. The K-means algorithm works by first assuming the number of subgroups, or clusters, in the data and then assigns each observation to one of those subgroups. In the next video, we will go deeper into how the k-means algorithm works to achieve this goal. For example, one might hypothesize that this data shown on the screen contain 2 subgroups. The k-means algorithm would assign all points in the top right hand corner to one subgroup and all observations in the bottom left hand corner to the other subgroup.

### k-means clustering
```{r}
# Create the k-means model: km.out
km.out <- kmeans(iriss, centers=3, nstart=20)

# Inspect the result
summary(km.out)
```
### Results of kmeans()
The kmeans() function produces several outputs.

In this exercise, you will access the cluster component directly. This is useful anytime you need the cluster membership for each observation of the data used to build the clustering model. A future exercise will show an example of how this cluster membership might be used to help communicate the results of k-means modeling.

k-means models also have a print method to give a human friendly output of basic modeling results. This is available by using print() or simply typing the name of the model.
```{r}
# Print the cluster membership component of the model
km.out$cluster

# Print the km.out object
print(km.out)
```
### Visualizing and interpreting results of kmeans()
One of the more intuitive ways to interpret the results of k-means models is by plotting the data as a scatter plot and using color to label the samples' cluster membership. In this exercise, you will use the standard plot() function to accomplish this.

To create a scatter plot, you can pass data with two features (i.e., columns) to plot() with an extra argument col = km.out$cluster, which sets the color of each point in the scatter plot according to its cluster membership.
```{r}
# Scatter plot of x
plot(x=iriss, 
     col=km.out$cluster,
     xlab="")
# Scatter plot of x
plot(iriss, 
     col = km.out$cluster, 
     xlab="", ylab="", 
     main="k-means with 3 clusters")
```

## How k-means works and practical matters
*Model selection*
Because kmeans has a random component, it is run multiple times and the best solution is selected from the multiple runs. The 'kmeans' algorithm needs a measurement of model quality to determine the 'best' outcome of multiple runs. 'kmeans' in R uses the **total within cluster sum of squares** as that measurement. The 'kmeans' run with the MINIMUM total within cluster sum of squares is considered the BEST MODEL Total within cluster sum of squares is easy to calculate. For each cluster in the model and for each observation assigned to that cluster, *calculate the squared distance from the observation to the cluster center*. This is just the squared Euclidean distance from plane geometry class. Sum all of the squared distances calculated and that is the total within cluster sum of squares.

*Determining the best number of clusters*
If you don't know the number of subgroups within the data beforehand, there is a way to heuristically determine the number of clusters. You could use trial and error, but instead the best approach is to run 'kmeans' with 1 through some number of clusters, recording the total within cluster sum of squares for each number of clusters. This is then plotted with the number of clusters on the horizontal axis and the total within cluster sum of squares on the vertical axis — this type of plot is referred to as a scree plot.
There may be an 'elbow' in the plotted data, a place where the total within cluster sum of squares decreases much slower with the addition of another cluster. In the plot above, the elbow appears at 2 clusters. This value can then be used to approximate the number of clusters if it is not given or known beforehand.

Your task is to generate six kmeans() models on the data, plotting the results of each, in order to see the impact of random initializations on model results.
```{r}
# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on x with three clusters and one start
  km.out <- kmeans(iriss, centers=3, nstart=1)
  
  # Plot clusters
  plot(iriss, col = km.out$cluster, 
       main = km.out$tot.withinss, 
       xlab = "", ylab = "")
}
```

### Selecting number of clusters
If you do not know the number of clusters and need to determine it, you will need to run the algorithm multiple times, each time with a different number of clusters. From this, you can observe how a measure of model quality changes with the number of clusters.
In this exercise, you will run kmeans() multiple times to see how model quality changes as the number of clusters changes. Plots displaying this information help to determine the number of clusters and are often referred to as scree plots.
```{r}
# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(iriss, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Set k equal to the number of clusters corresponding to the elbow location
k <- 2  # 3 is probably OK, too
```
### Practical matters: working with real data (Pokemon Data)
The second part of this exercise includes plotting the outcomes of the clustering on two dimensions, or features, of the data. These features were chosen somewhat arbitrarily for this exercise. Think about how you would use plotting and clustering to communicate interesting groups of Pokemon to other people.
```{r}
# Initialize total within sum of squares error: wss
wss <- 0

# Look over 1 to 15 possible clusters
for (i in 1:15) {
  # Fit the model: km.out
  km.Pokemon <- kmeans(Pokemon, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.Pokemon$tot.withinss
}

# Produce a scree plot
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Select number of clusters
k <- 2

# Build model with k clusters: km.out
km.Pokemon <- kmeans(Pokemon, centers = 2, nstart = 20, iter.max = 50)

# View the resulting model
km.Pokemon

# Plot of Defense vs. Speed by cluster membership
plot(Pokemon[, c("Defense", "Speed")],
     col = km.Pokemon$cluster,
     main = paste("k-means clustering of Pokemon with", k, "clusters"),
     xlab = "Defense", ylab = "Speed")
```

# CHAPTER 2. HIRARCHICAL CLUSTERING
Hierarchical clustering is used when the number of clusters is not known ahead of time. This is different from kmeans clustering where you first have to specify the number of clusters and then execute the algorithm. There are two approaches to hierarchical clustering: *Agglomerative* or *bottom-up* and *Divisive* or *top-down*. This course will focus on bottom-up clustering.

*bottom-up* approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
*top-down* approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.

Performing hierarchical clustering in R requires only one parameter -- the distance between the observations. 

## Hierarchical clustering with results
```{r}
# Create hierarchical clustering model: hclust.out
hclust.out <- hclust(dist(iriss))

# Inspect the result
summary(hclust.out)
```
If you look at the summary of the hierarchical clustering model the output is somewhat technical and opaque. In all honesty it's not that useful of a summary. So we will be building a tree, a dendrogram, which represents the clustering process.

As described before, every observation is made a cluster. Then the closest two clusters are joined together into a single cluster. This is equivalent to two points being joined on the tree representation of the clusters.

This process continues, finding the closest two clusters and joining them into a new cluster. The distance between the clusters is represented as the height of the horizontal line on the dendrogram.

This algorithm continues until only one cluster is remaining. This also completes the tree representation, the dendrogram, of the results of the hierarchical clustering algorithm.

### Dendrogram plotting in R
```{r}
plot(hclust.out)
```
Choosing the number of clusters based on distance between the clusters is equivalent to drawing a line on the dendrogram at a height equal to the desired distance between clusters. This is done using the abline() function in R, using the 'h' parameter to specify the height to draw the line, and optionally a color for the line, using the parameter 'c-o-l'. Here I show the results of abline() with a horizontal red line. Specifying height of the line is the equivalent of specifying that you want clusters that are no further apart than that height.
```{r}
plot(hclust.out)+
  abline(h=1, col="red")
```
### Tree "cutting" in R
Finally, to make cluster assignments for each observation in the cluster, you can use the "cut tree" function in R. The "cut tree" function takes as its parameters the hierarchical cluster model and either the height at which to cut the dendrogram tree, the 'h' parameter, or the number of clusters you want to maintain, the 'k' parameter. The results are a vector with a numeric cluster assignment for each observation.
```{r}
# The h and k arguments to cutree() allow you to cut the tree based on a certain height h or a certain number of clusters k.
# by the height in the …
cutree(hclust.out, h=1)
# by the number of clusters you want 
cutree(hclust.out, k=2)
```

## Clustering linkage and practical matters
*How is the distance between clusters determined?*
*Complete Method:* In the complete method, the distance -- or similarity -- is determined pairwise between all observations in cluster 1 and cluster 2, and the largest distance is used as the distance between the clusters.
*Single Method:* the pairwise similarity is calculated between points in each cluster, and the smallest such similarity is used as the distance between the clusters
*Average Method:* This method uses the average of the pairwise similarities as the distance between the two clusters.
*Centroid Method:* The centroid of cluster 1 is calculated and the centroid of cluster 2 is calculated, and the distance between the clusters is the distance between the centroids.

*Scaling data!!!*
Many of the machine learning methods, including kmeans and hierarchical clustering, are sensitive to the data which is on different scales or units of measurement. To resolve this, the data is transformed through a linear transformation before performing clustering. This transformation subtracts the mean of a feature from each of the observations, and divides each feature observation by the standard deviation of the feature. This is sometimes referred to as normalization and has the effect of producing a population where the normalized feature has a mean of zero and a standard deviation of one. If you know any of the features are on different scales or units of measure, then it is customary to normalize all the features.
*If the means and standard deviations vary across the features scaling is in order.* 
```{r}
# Check if Scaling is necessarry 
colMeans(iriss)
apply(iriss, 2, sd)
```

*Balanced Trees*
Balanced trees are essential if you want an even number of observations assigned to each cluster. On the other hand, if you want to detect outliers, for example, an unbalanced tree is more desirable because pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters.

### Practical matters: scaling
```{r}
# View column means
colMeans(Pokemon)

# View column standard deviations
apply(Pokemon, 2, sd)

# Scale the data
Pokemon.scaled <- scale(Pokemon)
colMeans(Pokemon.scaled)
apply(Pokemon.scaled, 2, sd)
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(Pokemon.scaled), method = "complete")
```

### Comparing kmeans() and hclust()
```{r}
# Apply cutree() to hclust.pokemon: cut.pokemon
cut.pokemon <- cutree(hclust.pokemon, k = 3)

# Compare methods
table(km.Pokemon$cluster, cut.pokemon)
```

# CHAPTER 3. DIMENSIONALITY REDUCTION WITH PCA
Dimensionality reduction has two main goals: to find structure within features and to aid in visualization.
*PCA*
First, PCA will find a linear combination of the original features. A linear combination just means it takes some fraction of some or all of the features and adds them together. These new features are called principal components. Second, in those new features, PCA will maintain as much variance as it can from the original data for a given number of principal components. Third, the new features are uncorrelated, or orthogonal to each other.

The goal of PCA would be to find a lower dimensional representation of this data that maintains and describes the maximum amount of variance from the original data.


```{r}
iris %>%
  lm(formula = Sepal.Width ~ Sepal.Length)
iris %>%
  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species))+
  geom_point()+
  stat_smooth(method = "lm",
              formula = y ~ x,
              geom = "smooth")
```
### PCA using prcomp()
```{r}
# Perform scaled PCA: pr.out
pr.out <- prcomp(iris[,-5], scale=TRUE)
pr.out.pokemon <- prcomp(Pokemon)
# Inspect model output
summary(pr.out)
summary(pr.out.pokemon)
```
## Visualizing and interpreting PCA results
### Biplot
This plot shows all of the original observations as points plotted in the first two principal components. A biplot also shows the original features as vectors mapped onto the first two principal components. In this case, the original features of petal length and petal width are in the same direction in the first two principal components, indicating that these two features are correlated in the original data.
```{r}
biplot(pr.out)
# Here Petal.Width and Petal Length are the two original variables that have approximately the same loadings in the first two principal components.
```
### Scree plot
The second type of plot is a scree plot. Scree plots for PCA either show the proportion of variance explained by each principal component, as on the left hand side here. Or they show the cumulative percentage of variance explained as the number of principal components increases (until all of the original variance is explained when the number of principal components equals the number of features in the original data). This is shown on the right hand side.

### Variance explained
```{r}
# Variability of each principal component: pr.var
pr.var <- pr.out$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
```
### Visualize variance explained
```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
```

## Practical issues with PCA
There are three types of items that need to be considered to complete a successful principal components analysis. The first of these, is **dealing with scaling the data**. The second item that sometimes needs to be considered is what to do with observations that have missing data in one or more of the features. There are many ways to address this issue, with one of the simplest approaches being to not include, or drop, observations with missing data. A more complex approach to dealing with missing data is to estimate or impute the missing values. While I will not go into more detail on this item, I wanted you to be aware of these strategies. The third practical matter is **how to handle observations with features that are categories** -- that is, features that are not numerical. The first strategy is the simplest -- do not include the categorical features in modeling. The second strategy is more involved and requires using one of many methods to encode the categorical features as numbers. This is more detail than I want to cover in this introductory course, but wanted to make sure you were aware of them if the situation presents itself.
### Example with mtcars
```{r}
pr.mtcars <- prcomp(mtcars)
biplot(pr.mtcars)
pr.mtcars_scale <- prcomp(mtcars, center = T, scale. = TRUE)
biplot(pr.mtcars_scale)
```
### Practical issues: scaling
```{r}
# Mean of each variable
colMeans(Pokemon)

# Standard deviation of each variable
apply(Pokemon, 2, sd)

# PCA model with scaling: pr.with.scaling
pr.with.scaling <- prcomp(Pokemon, scale=T)

# PCA model without scaling: pr.without.scaling
pr.without.scaling <- prcomp(Pokemon)

# Create biplots of both for comparison
biplot(pr.with.scaling)
biplot(pr.without.scaling)
```

# CHAPTER 4. CASE STUDY
The dataset you will be using in this analysis was published in a paper by Bennett and Mangasarian. Their data consisted of measurements of nuclei of cells of human breast masses. Each observation, or row, is of a single mass or group of cells and consists of ten features. Each feature is a summary statistic of measurements from the cells in that mass. There is also a target variable, or label, in the dataset. The label would be used if you were doing modeling using supervised learning -- it will not be used for modeling during this analysis.
At a high level you will complete six steps during this analysis. Downloading and preparing the data for modeling, doing some high level exploratory analysis, performing principal component analysis and using visualizations and other mechanisms to interpret the results, completing two types of clustering, understanding and comparing the two types, and finally, combining Principal Component Analysis as a preprocessing step to clustering. During the coding exercises, you will be guided through each step.

### Preparing the data
```{r}
url <- "https://assets.datacamp.com/production/course_1903/datasets/WisconsinCancer.csv"

# Download the data: wisc.df
wisc.df <- read.csv(url)

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```
```{r}
#How many observations are in this dataset?
dim(wisc.data)
#How many variables/features in the data are suffixed with _mean?
wisc.data %>%
  colnames(prefix = "_mean")
#How many of the observations have a malignant diagnosis?
wisc.df %>%
  count(diagnosis=="M")
```
### Performing PCA
You saw in the last chapter that it's important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data:

The input variables use different units of measurement.
The input variables have significantly different variances.
```{r}
# Check column means and standard deviations
round(colMeans(wisc.data), 2)
round(apply(wisc.data, 2, sd), 2)

# Execute PCA, scaling if appropriate: wisc.pr
wisc.pr <- prcomp(wisc.data, scale=T)

# Look at summary of results
summary(wisc.pr)
```
### Interpreting PCA results
```{r}
# Create a biplot of wisc.pr
biplot(wisc.pr)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")

# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```
### Variance explained
```{r}
# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- wisc.pr$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```
## PCA review and next steps
The first exercise on hierarchical clustering also has you compare the results of the clustering to the diagnosis -- if you were doing supervised learning, this step would provide you insights as to if the clusters would be useful features. Next, there is a comparison of the results of the two types of clustering; this type of work is done to contrast the results of the two algorithms and see if they produce similar or different sub-groupings.The first exercise on hierarchical clustering also has you compare the results of the clustering to the diagnosis -- if you were doing supervised learning, this step would provide you insights as to if the clusters would be useful features. Next, there is a comparison of the results of the two types of clustering; this type of work is done to contrast the results of the two algorithms and see if they produce similar or different sub-groupings. Finally, you'll combine PCA and clustering. PCA is often used as a preprocessing step for different types of machine learning -- when done that way it creates a type of regularization that helps avoid overfitting the data. In a coming exercise, you will see how PCA affects the results of clustering.

### Communicating PCA results
```{r}
#For the first principal component, what is the component of the loading vector for the feature concave.points_mean? 
wisc.pr$rotation["concave.points_mean", 1:3]
# Answer: -0.26085376
#What is the minimum number of principal components required to explain 80% of the variance of the data?
summary(wisc.pr)
# 5
```
### Hierarchical clustering of case data
```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)

# Create a hierarchical clustering model: wisc.hclust
wisc.hclust <- hclust(data.dist, method = "complete")
```
### Results of hierarchical clustering
Let's use the hierarchical clustering model you just created to determine a height (or distance between clusters) where a certain number of clusters exists.
```{r}
# Using the plot() function, what is the height at which the clustering model has 4 clusters?
plot(wisc.hclust)+ abline(h=20, col="red")
```
### Selecting number of clusters
```{r}
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

# Compare cluster membership to actual diagnoses
table(wisc.hclust.clusters, diagnosis)
```

### k-means clustering and comparing results
In this exercise, you will create a k-means clustering model on the Wisconsin breast cancer data and compare the results to the actual diagnoses and the results of your hierarchical clustering model. Take some time to see how each clustering model performs in terms of separating the two diagnoses and how the clustering models compare to each other.
```{r}
# Create a k-means model on wisc.data: wisc.km
wisc.data <- scale(wisc.data)
wisc.km <- kmeans(wisc.data, centers=2, nstart=20)
# Compare k-means to actual diagnoses
table(wisc.km$cluster, diagnosis)

# Compare k-means to hierarchical clustering
table(wisc.km$cluster, wisc.hclust.clusters)
```

### Clustering on PCA results
Recall from earlier exercises that the PCA model required significantly fewer features to describe 80% and 95% of the variability of the data. In addition to normalizing data and potentially avoiding overfitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques.

Let's see if PCA improves or degrades the performance of hierarchical clustering.
```{r}
# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method ="complete")

# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
# Compare to actual diagnoses
table(diagnosis, wisc.pr.hclust.clusters)
table(diagnosis, wisc.hclust.clusters)
# Compare to k-means and hierarchical
table(diagnosis, wisc.km$cluster, wisc.pr.hclust.clusters)
table(diagnosis, wisc.km$cluster)
```
